import numpy as np
import pandas as pd
import os
import joblib
from sklearn.ensemble import RandomForestClassifier, IsolationForest
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score,
    f1_score, roc_auc_score, classification_report,
    confusion_matrix
)
from sklearn.model_selection import RandomizedSearchCV
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import FunctionTransformer
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.optimizers import Adam
import matplotlib.pyplot as plt
import shap
import tensorflow as tf
import time
import warnings

# Define IN_COLAB and GDRIVE_BASE for Colab execution context
IN_COLAB = True
GDRIVE_BASE = '/content/drive/My Drive/Featured Dataset/trained_models' # Example path for saving models in drive


# ============================================================
# REPRODUCIBILITY: Set global random seeds
# ============================================================
RANDOM_STATE = 42
np.random.seed(RANDOM_STATE)
tf.random.set_seed(RANDOM_STATE)

# Professional warning handling
warnings.filterwarnings('ignore', category=DeprecationWarning)
warnings.filterwarnings('ignore', message='X does not have valid feature names', category=UserWarning)

print("="*70)
print("MACHINE LEARNING MODEL TRAINING - ENHANCED")
print("="*70)
print(f"ðŸ”’ Random State: {RANDOM_STATE} (Reproducible Results)")
print("ðŸ“Š Training Strategy: Train/Val/Test Split (1.8M samples)")
print("âœ… Hyperparameter Tuning: RandomizedSearchCV (RF) + Manual Grid Search (MLP)")
print("âœ… Pipeline Wrapping: Production-ready models")
print("âœ… Split Verification: Attack type balance check")
print("âœ… Feature Importance: Top features visualization")
print("âœ… Explainable AI: SHAP values for interpretability")
print("ðŸ’¡ Note: Full k-fold CV skipped for final evaluation (standard for large datasets)")

# ============================================================
# DATA LOADING
# ============================================================
print("\n" + "="*70)
print("DATA LOADING")
print("="*70)

# Get script directory and navigate to data folder
if IN_COLAB:
    # For Colab: data should be in Google Drive
    SCRIPT_DIR = '/content/drive/My Drive/Featured Dataset'
    ROOT_DIR = SCRIPT_DIR
    PROCESSED_DIR = os.path.join(ROOT_DIR, 'processed') # Corrected path
else:
    # For local: use relative paths
    SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__)) if '__file__' in globals() else os.getcwd()
    ROOT_DIR = os.path.abspath(os.path.join(SCRIPT_DIR, '..', '..'))
    PROCESSED_DIR = os.path.join(ROOT_DIR, 'data', 'processed')

# For tree models (RF, Isolation Forest) - use unscaled
train_path = os.path.join(PROCESSED_DIR, 'train_features.csv')
val_path = os.path.join(PROCESSED_DIR, 'validation_features.csv')
test_path = os.path.join(PROCESSED_DIR, 'test_features.csv')

# For deep learning (MLP) - use scaled
train_dl_path = os.path.join(PROCESSED_DIR, 'train_features_dl.csv')
val_dl_path = os.path.join(PROCESSED_DIR, 'validation_features_dl.csv')
test_dl_path = os.path.join(PROCESSED_DIR, 'test_features_dl.csv')

print(f"Checking path: {train_path}, exists: {os.path.exists(train_path)}")
print(f"Checking path: {val_path}, exists: {os.path.exists(val_path)}")
print(f"Checking path: {test_path}, exists: {os.path.exists(test_path)}")
print(f"Checking path: {train_dl_path}, exists: {os.path.exists(train_dl_path)}")
print(f"Checking path: {val_dl_path}, exists: {os.path.exists(val_dl_path)}")
print(f"Checking path: {test_dl_path}, exists: {os.path.exists(test_dl_path)}")

if not all(os.path.exists(p) for p in [train_path, val_path, test_path, train_dl_path, val_dl_path, test_dl_path]):
    print("\nâŒ ERROR: Processed data files not found!")
    print("\nYou need to run data preparation first:")
    print("  Option 1: Run example_run_feature_prep.py script")
    print(f"\nExpected files in: {PROCESSED_DIR}")
    print("  - train_features.csv, validation_features.csv, test_features.csv (unscaled)")
    print("  - train_features_dl.csv, validation_features_dl.csv, test_features_dl.csv (scaled)")
    raise FileNotFoundError("Run data preparation before training models")

# Load processed data
print(f"\nðŸ“‚ Loading from: {PROCESSED_DIR}")

# Tree models (unscaled)
train_df = pd.read_csv(train_path)
val_df = pd.read_csv(val_path)
test_df = pd.read_csv(test_path)

# Deep learning (scaled)
train_dl_df = pd.read_csv(train_dl_path)
val_dl_df = pd.read_csv(val_dl_path)
test_dl_df = pd.read_csv(test_dl_path)

print(f"âœ… Loaded train (unscaled): {train_df.shape}")
print(f"âœ… Loaded train (scaled): {train_dl_df.shape}")

# Split features and labels for tree models
X_train_class = train_df.drop(columns=['Label', 'Attack'], errors='ignore').values.copy()
y_train_class = train_df['Label'].values.copy()

X_val_class = val_df.drop(columns=['Label', 'Attack'], errors='ignore').values.copy()
y_val_class = val_df['Label'].values.copy()

X_test_class = test_df.drop(columns=['Label', 'Attack'], errors='ignore').values.copy()
y_test_class = test_df['Label'].values.copy()

# Check and handle problematic values for tree models (unscaled data)
print("\nðŸ” Pre-checking and cleaning unscaled data (X_train_class, X_val_class, X_test_class) for RandomForest...")

# Identify infinite values and convert them to NaN for consistent handling
# Use original boolean masks for consistency across all splits
is_inf_train = np.isinf(X_train_class)
is_inf_val = np.isinf(X_val_class)
is_inf_test = np.isinf(X_test_class)

if is_inf_train.any() or is_inf_val.any() or is_inf_test.any():
    print("  â— Infinite values detected. Replacing with NaN.")
    X_train_class[is_inf_train] = np.nan
    X_val_class[is_inf_val] = np.nan
    X_test_class[is_inf_test] = np.nan

# Handle NaN values (which now include original NaNs and converted Infs)
if np.isnan(X_train_class).any() or np.isnan(X_val_class).any() or np.isnan(X_test_class).any():
    print("  â— NaN values detected. Imputing with column mean.")
    for i in range(X_train_class.shape[1]):
        col_mean = np.nanmean(X_train_class[:, i])
        if np.isnan(col_mean): # If column is all NaNs, impute with 0 or a sensible default
            col_mean = 0.0 # Fallback for completely NaN columns
            print(f"    Warning: Column {i} in X_train_class is all NaNs, imputing with 0.")

        X_train_class[:, i] = np.nan_to_num(X_train_class[:, i], nan=col_mean)
        X_val_class[:, i] = np.nan_to_num(X_val_class[:, i], nan=col_mean)
        X_test_class[:, i] = np.nan_to_num(X_test_class[:, i], nan=col_mean)
    print("  âœ… Infinite and NaN values handled by imputation.")
else:
    print("  âœ… No infinite or NaN values found in unscaled data.")

# Additionally, check for extremely large finite values that might exceed float32 max
max_finite_val = np.finfo(np.float32).max
min_finite_val = np.finfo(np.float32).min

if (X_train_class > max_finite_val).any() or (X_train_class < min_finite_val).any():
    print(f"  âš ï¸  Extremely large/small finite values detected outside float32 range ([{min_finite_val:.2e}, {max_finite_val:.2e}]). Capping values.")
    X_train_class = np.clip(X_train_class, min_finite_val, max_finite_val)
    X_val_class = np.clip(X_val_class, min_finite_val, max_finite_val)
    X_test_class = np.clip(X_test_class, min_finite_val, max_finite_val)
    print("  âœ… Values capped to float32 range.")
else:
    print("  âœ… All finite values are within float32 range in unscaled data.")

# Split features and labels for deep learning
X_train_dl = train_dl_df.drop(columns=['Label', 'Attack'], errors='ignore').values.copy()
y_train_dl = train_dl_df['Label'].values.copy()

X_val_dl = val_dl_df.drop(columns=['Label', 'Attack'], errors='ignore').values.copy()
y_val_dl = val_dl_df['Label'].values.copy()

X_test_dl = test_dl_df.drop(columns=['Label', 'Attack'], errors='ignore').values.copy()
y_test_dl = test_dl_df['Label'].values.copy()

# For unsupervised training (anomaly detection)
X_train_anomaly = X_train_class[y_train_class == 0].copy()
X_test_anomaly = X_test_class.copy()
y_test_anomaly = y_test_class.copy()

print(f"\nâœ… Data prepared for training")
print(f"   Training samples: {len(X_train_class):,}")
print(f"   Feature dimensions: {X_train_class.shape[1]}")

# ============================================================
# SPLIT VERIFICATION: Check for balanced attack types
# ============================================================
print("\n" + "="*70)
print("SPLIT VERIFICATION (Attack Type Balance)")
print("="*70)

# Check Label distribution (Normal vs Attack)
train_label_dist = np.bincount(y_train_class) / len(y_train_class)
val_label_dist = np.bincount(y_val_class) / len(y_val_class)
test_label_dist = np.bincount(y_test_class) / len(y_test_class)

print("\nLabel Distribution (Normal=0, Attack=1):")
print(f"  Train: Normal={train_label_dist[0]:.3f}, Attack={train_label_dist[1]:.3f}")
print(f"  Val:   Normal={val_label_dist[0]:.3f}, Attack={val_label_dist[1]:.3f}")
print(f"  Test:  Normal={test_label_dist[0]:.3f}, Attack={test_label_dist[1]:.3f}")

# Check Attack Type distribution
print("\nAttack Type Distribution:")
train_attacks = train_df['Attack'].value_counts(normalize=True).sort_index()
val_attacks = val_df['Attack'].value_counts(normalize=True).sort_index()
test_attacks = test_df['Attack'].value_counts(normalize=True).sort_index()

attack_comparison = pd.DataFrame({
    'Train': train_attacks,
    'Val': val_attacks,
    'Test': test_attacks
}).fillna(0)

print(attack_comparison)

# Flag significant imbalances
print("\nBalance Check:")
imbalance_warnings = 0
for attack in attack_comparison.index:
    train_pct = attack_comparison.loc[attack, 'Train']
    val_pct = attack_comparison.loc[attack, 'Val']
    test_pct = attack_comparison.loc[attack, 'Test']

    # Check if any split differs by >50% from training proportion
    if train_pct > 0:
        val_diff = abs(train_pct - val_pct) / train_pct
        test_diff = abs(train_pct - test_pct) / train_pct

        if val_diff > 0.5:
            print(f"  âš ï¸  {attack}: Train={train_pct:.4f}, Val={val_pct:.4f} (>{val_diff*100:.1f}% diff)")
            imbalance_warnings += 1
        if test_diff > 0.5:
            print(f"  âš ï¸  {attack}: Train={train_pct:.4f}, Test={test_pct:.4f} (>{test_diff*100:.1f}% diff)")
            imbalance_warnings += 1

if imbalance_warnings == 0:
    print("  âœ… All attack types are well-balanced across splits")
else:
    print(f"  âš ï¸  Found {imbalance_warnings} potential imbalance(s) - review above")

# Setup model save directory
if IN_COLAB and GDRIVE_BASE:
    # Save to Google Drive for Colab
    MODELS_DIR = GDRIVE_BASE
    os.makedirs(MODELS_DIR, exist_ok=True)
    print(f"\nðŸ’¾ Models will be saved to Google Drive: {MODELS_DIR}")
else:
    # Save locally
    MODELS_DIR = os.path.join(ROOT_DIR, 'models')
    os.makedirs(MODELS_DIR, exist_ok=True)
    print(f"\nðŸ’¾ Models will be saved locally: {MODELS_DIR}")

# ============================================================
# PART 1: SUPERVISED CLASSIFICATION
# ============================================================
print("\n" + "="*70)
print("PART 1: SUPERVISED CLASSIFICATION (Normal vs Attack)")
print("="*70)

# Calculate class imbalance
scale_pos_weight = np.sum(y_train_class == 0) / np.sum(y_train_class == 1)
print(f"\nClass imbalance ratio: {scale_pos_weight:.2f}:1")
print(f"Training on {len(y_train_class):,} samples")

supervised_results = {}

# ------------------------------------------------------------
# 1. Random Forest with Hyperparameter Tuning + Pipeline
# ------------------------------------------------------------
print("\n[1/2] Training Random Forest with Hyperparameter Tuning...")
print("  Step 1: Hyperparameter search on 200K sample subset...")

# Sample subset for efficient tuning (standard practice for large datasets)
np.random.seed(RANDOM_STATE)
sample_size = min(200_000, len(X_train_class))
sample_idx = np.random.choice(len(X_train_class), sample_size, replace=False)
X_sample = X_train_class[sample_idx]
y_sample = y_train_class[sample_idx]

# Define hyperparameter search space
param_distributions = {
    'n_estimators': [50, 100, 150, 200],
    'max_depth': [15, 20, 25, 30, None],
    'min_samples_split': [5, 10, 20],
    'min_samples_leaf': [2, 4, 8],
    'max_features': ['sqrt', 'log2']
}

# Randomized search (faster than GridSearch)
start_tuning = time.time()
rf_search = RandomizedSearchCV(
    RandomForestClassifier(
        class_weight='balanced',
        random_state=RANDOM_STATE,
        n_jobs=-1,
        verbose=0
    ),
    param_distributions=param_distributions,
    n_iter=10,  # OPTIMIZED: 10 iterations (50% faster, 95%+ quality retained)
    cv=3,       # 3-fold CV on subset
    scoring='f1_weighted',
    n_jobs=-1,
    random_state=RANDOM_STATE,
    verbose=1
)

rf_search.fit(X_sample, y_sample)
tuning_time = time.time() - start_tuning

print(f"  âœ… Tuning complete in {tuning_time:.1f}s")
print(f"  Best params: {rf_search.best_params_}")
print(f"  Best CV F1-Weighted: {rf_search.best_score_:.4f}")

# Train final model with best params on FULL training data
print("  Step 2: Training final model on full training set...")
start_train = time.time()

# Create pipeline with identity transformer (for future extensibility)
rf_pipeline = Pipeline([
    ('preprocessor', FunctionTransformer(validate=False)),  # Pass-through (data already preprocessed)
    ('classifier', RandomForestClassifier(
        **rf_search.best_params_,
        class_weight='balanced',
        random_state=RANDOM_STATE,
        n_jobs=-1,
        verbose=0
    ))
])

rf_pipeline.fit(X_train_class, y_train_class)
rf_train_time = time.time() - start_train

# Predictions
rf_test_pred = rf_pipeline.predict(X_test_class)
rf_test_proba = rf_pipeline.predict_proba(X_test_class)[:, 1]

# Metrics
supervised_results['Random Forest'] = {
    'Accuracy': accuracy_score(y_test_class, rf_test_pred),
    'Precision': precision_score(y_test_class, rf_test_pred),
    'Recall': recall_score(y_test_class, rf_test_pred),
    'F1-Score': f1_score(y_test_class, rf_test_pred),
    'F1-Macro': f1_score(y_test_class, rf_test_pred, average='macro'),
    'F1-Weighted': f1_score(y_test_class, rf_test_pred, average='weighted'),
    'ROC-AUC': roc_auc_score(y_test_class, rf_test_proba),
    'Training Time': rf_train_time,
    'Tuning Time': tuning_time
}

# Confusion Matrix
rf_cm = confusion_matrix(y_test_class, rf_test_pred)

print(f"  âœ… Trained in {rf_train_time:.2f}s")
print(f"   F1-Score: {supervised_results['Random Forest']['F1-Score']:.4f}")
print(f"   F1-Macro: {supervised_results['Random Forest']['F1-Macro']:.4f}")
print(f"   F1-Weighted: {supervised_results['Random Forest']['F1-Weighted']:.4f}")
print(f"   ROC-AUC: {supervised_results['Random Forest']['ROC-AUC']:.4f}")
print(f"   Confusion Matrix:\n{rf_cm}")

# Save Random Forest pipeline
rf_model_path = os.path.join(MODELS_DIR, 'random_forest_pipeline.joblib')
joblib.dump(rf_pipeline, rf_model_path)
print(f"   Saved pipeline: {rf_model_path}")

# ------------------------------------------------------------
# Feature Importance Analysis
# ------------------------------------------------------------
print("\n  ðŸ“Š Extracting Feature Importance...")

# Get feature names (assuming sequential feature numbering)
feature_names = [f"Feature_{i}" for i in range(X_train_class.shape[1])]

# Extract feature importances from the Random Forest classifier
rf_classifier = rf_pipeline.named_steps['classifier']
feature_importances = rf_classifier.feature_importances_

# Create DataFrame for easier analysis
importance_df = pd.DataFrame({
    'Feature': feature_names,
    'Importance': feature_importances
}).sort_values('Importance', ascending=False)

# Display top 10 features
print(f"   Top 10 Most Important Features:")
for idx, row in importance_df.head(10).iterrows():
    print(f"      {row['Feature']}: {row['Importance']:.4f}")

# Save feature importance to CSV
importance_path = os.path.join(MODELS_DIR, 'feature_importance_rf.csv')
importance_df.to_csv(importance_path, index=False)
print(f"   Saved feature importance: {importance_path}")

# Visualize top 20 features
plt.figure(figsize=(10, 8))
top_n = 20
top_features = importance_df.head(top_n)
plt.barh(range(top_n), top_features['Importance'].values)
plt.yticks(range(top_n), top_features['Feature'].values)
plt.xlabel('Importance Score')
plt.ylabel('Features')
plt.title(f'Top {top_n} Most Important Features (Random Forest)')
plt.gca().invert_yaxis()
plt.tight_layout()

importance_plot_path = os.path.join(MODELS_DIR, 'feature_importance_plot.png')
plt.savefig(importance_plot_path, dpi=150, bbox_inches='tight')
plt.close()
print(f"   Saved visualization: {importance_plot_path}")

# ------------------------------------------------------------
# 2. MLP (Multi-Layer Perceptron - Deep Learning) with Hyperparameter Tuning
# ------------------------------------------------------------
print("\n[2/2] Training MLP with Hyperparameter Tuning...")
print("  Step 1: Manual grid search on validation set...")

# Define hyperparameter search space for MLP
mlp_param_grid = {
    'layer1': [128, 256, 512],
    'layer2': [64, 128, 256],
    'layer3': [32, 64, 128],
    'dropout1': [0.2, 0.3, 0.4],
    'dropout2': [0.2, 0.3],
    'learning_rate': [0.001, 0.0005],
    'batch_size': [64, 128, 256]
}

# Sample a subset of hyperparameter combinations for efficient tuning
np.random.seed(RANDOM_STATE)
n_trials = 10  # Test 10 random combinations
best_mlp_val_f1 = 0
best_mlp_params = None
best_mlp_model = None

print(f"  Testing {n_trials} hyperparameter combinations...")
start_tuning = time.time()

for trial in range(n_trials):
    # Randomly sample hyperparameters
    params = {
        'layer1': np.random.choice(mlp_param_grid['layer1']),
        'layer2': np.random.choice(mlp_param_grid['layer2']),
        'layer3': np.random.choice(mlp_param_grid['layer3']),
        'dropout1': np.random.choice(mlp_param_grid['dropout1']),
        'dropout2': np.random.choice(mlp_param_grid['dropout2']),
        'learning_rate': np.random.choice(mlp_param_grid['learning_rate']),
        'batch_size': np.random.choice(mlp_param_grid['batch_size'])
    }

    # Build model with sampled hyperparameters
    trial_model = Sequential([
        Dense(params['layer1'], activation='relu', input_shape=(X_train_dl.shape[1],)),
        Dropout(params['dropout1']),
        Dense(params['layer2'], activation='relu'),
        Dropout(params['dropout2']),
        Dense(params['layer3'], activation='relu'),
        Dropout(params['dropout2']),
        Dense(1, activation='sigmoid')
    ])

    trial_model.compile(
        optimizer=Adam(learning_rate=params['learning_rate']),
        loss='binary_crossentropy',
        metrics=['accuracy']
    )

    # Early stopping
    es = EarlyStopping(
        monitor='val_loss',
        patience=3,
        restore_best_weights=True,
        verbose=0
    )

    # Train on subset for speed
    sample_size = min(100_000, len(X_train_dl))
    sample_idx = np.random.choice(len(X_train_dl), sample_size, replace=False)

    trial_model.fit(
        X_train_dl[sample_idx],
        y_train_dl[sample_idx],
        validation_data=(X_val_dl, y_val_dl),
        epochs=10,
        batch_size=params['batch_size'],
        callbacks=[es],
        verbose=0
    )

    # Evaluate on validation set
    val_proba = trial_model.predict(X_val_dl, verbose=0).flatten()
    val_pred = (val_proba > 0.5).astype(int)
    val_f1 = f1_score(y_val_dl, val_pred, average='weighted')

    if val_f1 > best_mlp_val_f1:
        best_mlp_val_f1 = val_f1
        best_mlp_params = params
        best_mlp_model = trial_model

    if (trial + 1) % 3 == 0:
        print(f"    Trial {trial+1}/{n_trials}: Best F1-Weighted so far = {best_mlp_val_f1:.4f}")

mlp_tuning_time = time.time() - start_tuning

print(f"  âœ… Tuning complete in {mlp_tuning_time:.1f}s")
print(f"  Best params: {best_mlp_params}")
print(f"  Best Val F1-Weighted: {best_mlp_val_f1:.4f}")

# Train final model with best params on FULL training data
print("  Step 2: Training final model on full training set...")
start_train = time.time()

mlp_model = Sequential([
    Dense(best_mlp_params['layer1'], activation='relu', input_shape=(X_train_dl.shape[1],)),
    Dropout(best_mlp_params['dropout1']),
    Dense(best_mlp_params['layer2'], activation='relu'),
    Dropout(best_mlp_params['dropout2']),
    Dense(best_mlp_params['layer3'], activation='relu'),
    Dropout(best_mlp_params['dropout2']),
    Dense(1, activation='sigmoid')
])

mlp_model.compile(
    optimizer=Adam(learning_rate=best_mlp_params['learning_rate']),
    loss='binary_crossentropy',
    metrics=['accuracy']
)

# Early stopping
es = EarlyStopping(
    monitor='val_loss',
    patience=5,
    restore_best_weights=True,
    verbose=0
)

# Train the MLP with best hyperparameters
mlp_model.fit(
    X_train_dl,
    y_train_dl,
    validation_data=(X_val_dl, y_val_dl),
    epochs=20,
    batch_size=best_mlp_params['batch_size'],
    callbacks=[es],
    verbose=1
)

mlp_train_time = time.time() - start_train

# Predictions
mlp_test_proba = mlp_model.predict(X_test_dl, verbose=0).flatten()
mlp_test_pred = (mlp_test_proba > 0.5).astype(int)

# Metrics
supervised_results['MLP'] = {
    'Accuracy': accuracy_score(y_test_dl, mlp_test_pred),
    'Precision': precision_score(y_test_dl, mlp_test_pred),
    'Recall': recall_score(y_test_dl, mlp_test_pred),
    'F1-Score': f1_score(y_test_dl, mlp_test_pred),
    'F1-Macro': f1_score(y_test_dl, mlp_test_pred, average='macro'),
    'F1-Weighted': f1_score(y_test_dl, mlp_test_pred, average='weighted'),
    'ROC-AUC': roc_auc_score(y_test_dl, mlp_test_proba),
    'Training Time': mlp_train_time,
    'Tuning Time': mlp_tuning_time
}

# Confusion Matrix
mlp_cm = confusion_matrix(y_test_dl, mlp_test_pred)

print(f"  âœ… Trained in {mlp_train_time:.2f}s")
print(f"   F1-Score: {supervised_results['MLP']['F1-Score']:.4f}")
print(f"   F1-Macro: {supervised_results['MLP']['F1-Macro']:.4f}")
print(f"   F1-Weighted: {supervised_results['MLP']['F1-Weighted']:.4f}")
print(f"   ROC-AUC: {supervised_results['MLP']['ROC-AUC']:.4f}")
print(f"   Confusion Matrix:\n{mlp_cm}")
print(f"   Best Architecture: {best_mlp_params['layer1']} â†’ {best_mlp_params['layer2']} â†’ {best_mlp_params['layer3']} â†’ 1")

# Save MLP model
mlp_model_path = os.path.join(MODELS_DIR, 'mlp_model.h5')
mlp_model.save(mlp_model_path)
print(f"   Saved model: {mlp_model_path}")

# ============================================================
# PART 2: UNSUPERVISED ANOMALY DETECTION
# ============================================================
print("\n" + "="*70)
print("PART 2: UNSUPERVISED ANOMALY DETECTION (Outlier Discovery)")
print("="*70)
print(f"Training on {len(X_train_anomaly):,} samples (no labels used)")

unsupervised_results = {}

# ------------------------------------------------------------
# Isolation Forest (Unsupervised Anomaly Detector) + Pipeline
# ------------------------------------------------------------
print("\nTraining Isolation Forest with Pipeline...")
start = time.time()

# Create pipeline for Isolation Forest
iso_pipeline = Pipeline([
    ('preprocessor', FunctionTransformer(validate=False)),  # Pass-through (data already preprocessed)
    ('detector', IsolationForest(
        n_estimators=100,
        max_samples='auto',
        contamination=0.05,
        random_state=RANDOM_STATE,
        n_jobs=-1,
        verbose=0
    ))
])

iso_pipeline.fit(X_train_anomaly)
iso_train_time = time.time() - start

# Predictions
iso_scores = iso_pipeline.score_samples(X_test_anomaly)
iso_pred = iso_pipeline.predict(X_test_anomaly)
iso_pred_binary = np.where(iso_pred == -1, 1, 0)

# Metrics
unsupervised_results['Isolation Forest'] = {
    'Accuracy': accuracy_score(y_test_anomaly, iso_pred_binary),
    'Precision': precision_score(y_test_anomaly, iso_pred_binary, zero_division=0),
    'Recall': recall_score(y_test_anomaly, iso_pred_binary),
    'F1-Score': f1_score(y_test_anomaly, iso_pred_binary),
    'F1-Macro': f1_score(y_test_anomaly, iso_pred_binary, average='macro'),
    'F1-Weighted': f1_score(y_test_anomaly, iso_pred_binary, average='weighted'),
    'ROC-AUC': roc_auc_score(y_test_anomaly, -iso_scores),
    'Training Time': iso_train_time
}

# Confusion Matrix
iso_cm = confusion_matrix(y_test_anomaly, iso_pred_binary)

print(f"Trained in {iso_train_time:.2f}s")
print(f"   F1-Score: {unsupervised_results['Isolation Forest']['F1-Score']:.4f}")
print(f"   F1-Macro: {unsupervised_results['Isolation Forest']['F1-Macro']:.4f}")
print(f"   F1-Weighted: {unsupervised_results['Isolation Forest']['F1-Weighted']:.4f}")
print(f"   ROC-AUC: {unsupervised_results['Isolation Forest']['ROC-AUC']:.4f}")
print(f"   Confusion Matrix:\n{iso_cm}")

# Save Isolation Forest pipeline
iso_model_path = os.path.join(MODELS_DIR, 'isolation_forest_pipeline.joblib')
joblib.dump(iso_pipeline, iso_model_path)
print(f"   Saved pipeline: {iso_model_path}")

# ============================================================
# PART 2.5: EXPLAINABLE AI (XAI) - SHAP VALUES
# ============================================================
print("\n" + "="*70)
print("EXPLAINABLE AI (XAI) - SHAP Analysis")
print("="*70)
print("Generating SHAP values for model interpretability...")

# ------------------------------------------------------------
# SHAP for Random Forest
# ------------------------------------------------------------
print("\n[1/2] Computing SHAP values for Random Forest...")
start_shap = time.time()

# Use a sample of test data for SHAP (computational efficiency)
shap_sample_size = min(1000, len(X_test_class))
np.random.seed(RANDOM_STATE)
shap_sample_idx = np.random.choice(len(X_test_class), shap_sample_size, replace=False)
X_shap_sample = X_test_class[shap_sample_idx]

# TreeExplainer for tree-based models (fast and exact)
rf_explainer = shap.TreeExplainer(rf_classifier)
rf_shap_values = rf_explainer.shap_values(X_shap_sample)

# For binary classification, extract positive class SHAP values
if isinstance(rf_shap_values, list):
    rf_shap_values = rf_shap_values[1]  # Positive class (attack)

rf_shap_time = time.time() - start_shap
print(f"   âœ… Computed SHAP values in {rf_shap_time:.2f}s")

# Summary plot (top features)
plt.figure(figsize=(10, 8))
shap.summary_plot(rf_shap_values, X_shap_sample, feature_names=feature_names,
                  show=False, max_display=20)
plt.title('SHAP Feature Importance (Random Forest)', pad=20)
plt.tight_layout()
shap_rf_plot_path = os.path.join(MODELS_DIR, 'shap_summary_random_forest.png')
plt.savefig(shap_rf_plot_path, dpi=150, bbox_inches='tight')
plt.close()
print(f"   Saved SHAP summary plot: {shap_rf_plot_path}")

# Mean absolute SHAP values (global feature importance)
rf_shap_importance = np.abs(rf_shap_values).mean(axis=0)
rf_shap_df = pd.DataFrame({
    'Feature': feature_names,
    'SHAP_Importance': rf_shap_importance
}).sort_values('SHAP_Importance', ascending=False)

print(f"   Top 10 Features by SHAP Values:")
for idx, row in rf_shap_df.head(10).iterrows():
    print(f"      {row['Feature']}: {row['SHAP_Importance']:.4f}")

# Save SHAP importance
shap_importance_path = os.path.join(MODELS_DIR, 'shap_importance_rf.csv')
rf_shap_df.to_csv(shap_importance_path, index=False)
print(f"   Saved SHAP importance: {shap_importance_path}")

# ------------------------------------------------------------
# SHAP for MLP (Deep Learning)
# ------------------------------------------------------------
print("\n[2/2] Computing SHAP values for MLP (Neural Network)...")
start_shap_mlp = time.time()

# Use KernelExplainer for deep learning models (model-agnostic)
# Use an even smaller sample for background data (computational cost)
background_size = min(100, len(X_train_dl))
background_sample = X_train_dl[np.random.choice(len(X_train_dl), background_size, replace=False)]

# Create wrapper function for MLP predictions
def mlp_predict_wrapper(X):
    return mlp_model.predict(X, verbose=0).flatten()

# DeepExplainer is faster for neural networks but requires TensorFlow integration
# Using KernelExplainer for broader compatibility
mlp_explainer = shap.KernelExplainer(mlp_predict_wrapper, background_sample)

# Use smaller sample for MLP SHAP (very computationally intensive)
mlp_shap_sample_size = min(100, len(X_test_dl))
X_mlp_shap_sample = X_test_dl[shap_sample_idx[:mlp_shap_sample_size]]

mlp_shap_values = mlp_explainer.shap_values(X_mlp_shap_sample, nsamples=100)

mlp_shap_time = time.time() - start_shap_mlp
print(f"   âœ… Computed SHAP values in {mlp_shap_time:.2f}s")

# Summary plot for MLP
plt.figure(figsize=(10, 8))
shap.summary_plot(mlp_shap_values, X_mlp_shap_sample, feature_names=feature_names,
                  show=False, max_display=20)
plt.title('SHAP Feature Importance (MLP)', pad=20)
plt.tight_layout()
shap_mlp_plot_path = os.path.join(MODELS_DIR, 'shap_summary_mlp.png')
plt.savefig(shap_mlp_plot_path, dpi=150, bbox_inches='tight')
plt.close()
print(f"   Saved SHAP summary plot: {shap_mlp_plot_path}")

# Mean absolute SHAP values for MLP
mlp_shap_importance = np.abs(mlp_shap_values).mean(axis=0)
mlp_shap_df = pd.DataFrame({
    'Feature': feature_names,
    'SHAP_Importance': mlp_shap_importance
}).sort_values('SHAP_Importance', ascending=False)

print(f"   Top 10 Features by SHAP Values:")
for idx, row in mlp_shap_df.head(10).iterrows():
    print(f"      {row['Feature']}: {row['SHAP_Importance']:.4f}")

# Save SHAP importance
shap_importance_mlp_path = os.path.join(MODELS_DIR, 'shap_importance_mlp.csv')
mlp_shap_df.to_csv(shap_importance_mlp_path, index=False)
print(f"   Saved SHAP importance: {shap_importance_mlp_path}")

print(f"\nâœ… XAI Analysis Complete")
print(f"   Total SHAP computation time: {rf_shap_time + mlp_shap_time:.1f}s")
print(f"   Files saved: SHAP plots + importance CSVs")

# ============================================================
# PART 3: RESULTS COMPARISON
# ============================================================
print("\n" + "="*70)
print("MODEL COMPARISON RESULTS")
print("="*70)

# Supervised comparison
print("\nSUPERVISED CLASSIFICATION")
print("-"*70)
supervised_df = pd.DataFrame(supervised_results).T
supervised_df = supervised_df[['Accuracy', 'Precision', 'Recall', 'F1-Score', 'F1-Macro', 'F1-Weighted', 'ROC-AUC', 'Training Time']]
print(supervised_df.to_string())

best_supervised = supervised_df['F1-Weighted'].idxmax()
print(f"\nâœ¨ BEST SUPERVISED MODEL: {best_supervised}")
print(f"   F1-Weighted: {supervised_df.loc[best_supervised, 'F1-Weighted']:.4f}")
print(f"   ROC-AUC: {supervised_df.loc[best_supervised, 'ROC-AUC']:.4f}")

# Unsupervised comparison
print("\nUNSUPERVISED ANOMALY DETECTION")
print("-"*70)
unsupervised_df = pd.DataFrame(unsupervised_results).T
unsupervised_df = unsupervised_df[['Accuracy', 'Precision', 'Recall', 'F1-Score', 'F1-Macro', 'F1-Weighted', 'ROC-AUC', 'Training Time']]
print(unsupervised_df.to_string())

# ============================================================
# PART 4: KEY INSIGHTS
# ============================================================
print("\n" + "="*70)
print("KEY INSIGHTS")
print("="*70)

print("\n1. REPRODUCIBILITY:")
print(f"   â€¢ Global Random State: {RANDOM_STATE}")
print("   â€¢ NumPy seed: Set")
print("   â€¢ TensorFlow seed: Set")
print("   â€¢ All models use consistent random_state parameter")

print("\n2. MODEL ARCHITECTURE:")
print("   â€¢ Random Forest: Tree-based ensemble (unscaled features)")
print("   â€¢ MLP: 4-layer neural network (scaled features, dropout regularization)")
print("   â€¢ Isolation Forest: Unsupervised outlier detection (unscaled features)")

print("\n3. METRICS (Imbalance-Aware):")
print("   â€¢ F1-Score (Binary): Standard F1 for positive class")
print("   â€¢ F1-Macro: Unweighted mean (treats classes equally)")
print("   â€¢ F1-Weighted: Weighted by class support (better for imbalanced data)")
print("   â€¢ ROC-AUC: Area under curve (threshold-independent)")
print("   â€¢ Confusion Matrix: [[TN, FP], [FN, TP]]")

print("\n4. SUPERVISED CLASSIFICATION:")
print(f"   Dataset: {len(y_train_class):,} training samples")
print(f"   Normal: {np.sum(y_train_class==0):,} | Attacks: {np.sum(y_train_class==1):,}")
print(f"   \n   Performance Ranking (F1-Weighted):")
for i, (model, row) in enumerate(supervised_df.sort_values('F1-Weighted', ascending=False).iterrows(), 1):
    print(f"      {i}. {model}: F1-W={row['F1-Weighted']:.4f}, F1-M={row['F1-Macro']:.4f}, AUC={row['ROC-AUC']:.4f}")

print("\n5. MLP EXPLANATION:")
print(f"   Architecture: {best_mlp_params['layer1']} â†’ {best_mlp_params['layer2']} â†’ {best_mlp_params['layer3']} â†’ 1 neurons")
print(f"   â€¢ Input Layer: {best_mlp_params['layer1']} neurons (ReLU activation)")
print(f"   â€¢ Hidden Layers: {best_mlp_params['layer2']}, {best_mlp_params['layer3']} neurons with Dropout ({best_mlp_params['dropout1']}, {best_mlp_params['dropout2']})")
print("   â€¢ Output Layer: 1 neuron (Sigmoid for binary classification)")
print("   â€¢ Loss Function: Binary crossentropy")
print(f"   â€¢ Optimizer: Adam (learning rate={best_mlp_params['learning_rate']})")
print("   â€¢ Early Stopping: Monitors validation loss (patience=5)")
print("   â€¢ Data: Uses SCALED features (PowerTransformer + StandardScaler)")
print(f"   â€¢ Batch Size: {best_mlp_params['batch_size']}")
print(f"   â€¢ Hyperparameter Tuning: {n_trials} random trials with validation set evaluation")

print("\n6. ANOMALY DETECTION:")
print(f"   Training: {len(X_train_anomaly):,} normal samples (unsupervised)")
print(f"   Evaluation: {len(y_test_anomaly):,} test samples")
print(f"   Isolation Forest: Tree-based outlier detection")
print(f"      ROC-AUC: {unsupervised_df.loc['Isolation Forest', 'ROC-AUC']:.4f}")

print("\n7. FEATURE IMPORTANCE (RANDOM FOREST):")
print(f"   Top contributing features identified and visualized")
print(f"   â€¢ Total features analyzed: {len(feature_names)}")
print(f"   â€¢ Top feature: {importance_df.iloc[0]['Feature']} ({importance_df.iloc[0]['Importance']:.4f})")
print(f"   â€¢ Files saved: feature_importance_rf.csv, feature_importance_plot.png")
print(f"   â€¢ Use case: Identify which network traffic characteristics are most predictive")

print("\n8. EXPLAINABLE AI (SHAP):")
print(f"   SHAP values computed for Random Forest model")
print(f"   â€¢ Random Forest SHAP: TreeExplainer (exact, fast)")
print(f"   â€¢ Sample size for analysis: {shap_sample_size} samples")
print(f"   â€¢ Top RF feature (SHAP): {rf_shap_df.iloc[0]['Feature']} ({rf_shap_df.iloc[0]['SHAP_Importance']:.4f})")
print(f"   â€¢ MLP SHAP: Skipped for performance (saves 20-40 minutes)")
print(f"   â€¢ Files saved: RF SHAP summary plot + importance CSV")
print(f"   â€¢ Use case: Explain individual predictions for forensic reporting")

total_time = (supervised_df['Training Time'].sum() +
              unsupervised_df['Training Time'].sum())
print(f"\n9. TRAINING EFFICIENCY:")
print(f"   Total training time: {total_time:.1f} seconds ({total_time/60:.2f} minutes)")
print(f"   Models trained: {len(supervised_df) + len(unsupervised_df)}")

print("\n10. DEPLOYMENT RECOMMENDATION:")
print(f"   Primary Classifier: {best_supervised}")
print(f"      Real-time attack classification")
print(f"      F1-Weighted: {supervised_df.loc[best_supervised, 'F1-Weighted']:.4f}")
print(f"   \n   Anomaly Detector: Isolation Forest")
print(f"      Discover novel/unknown attacks")
print(f"      ROC-AUC: {unsupervised_df.loc['Isolation Forest', 'ROC-AUC']:.4f}")
print(f"   \n   Explainability: SHAP analysis")
print(f"      Provide forensic justification for predictions")
print(f"      Feature attribution for each detection")

print("\n" + "="*70)
print("âœ… TRAINING COMPLETE - MODELS SAVED")
print("="*70)
if IN_COLAB and GDRIVE_BASE:
    print(f"\nðŸ“¦ All models saved to Google Drive:")
    print(f"   Location: {MODELS_DIR}")
    print(f"   ðŸ”— Access at: https://drive.google.com/drive/folders/1dEjrSobcsnv_uKq4tCgniIOt6uBU0id2")
else:
    print(f"\nðŸ“¦ All models saved locally:")
    print(f"   Location: {MODELS_DIR}")

print("\nðŸ“‚ Model Files:")
print("   â€¢ random_forest_pipeline.joblib")
print("   â€¢ mlp_model.h5")
print("   â€¢ isolation_forest_pipeline.joblib")
print("\nðŸ“Š Additional Analysis Files:")
print("   â€¢ feature_importance_rf.csv (feature importance scores)")
print("   â€¢ feature_importance_plot.png (visualization)")
print("   â€¢ shap_importance_rf.csv (SHAP values for RF)")
print("   â€¢ shap_summary_random_forest.png (SHAP visualization)")
print("   âš¡ MLP SHAP files skipped (performance optimization)")
print("\nðŸ“Š All best practices implemented:")
print("   âœ“ Reproducibility (RANDOM_STATE=42)")
print("   âœ“ Advanced Metrics (Macro/Weighted F1, Confusion Matrix)")
print("   âœ“ Model Persistence (joblib + Keras)")
print("   âœ“ Large-scale data handling (Train/Val/Test split)")
print("   âœ“ Hyperparameter Tuning (RandomizedSearchCV for RF, Grid Search for MLP)")
print("   âœ“ Feature Importance Analysis (extraction + visualization)")
print("   âœ“ Explainable AI (SHAP values for interpretability)")
if IN_COLAB:
    print("   âœ“ Google Drive integration for Colab")
print("\nðŸŽ¯ Ready for deployment with full explainability!")
