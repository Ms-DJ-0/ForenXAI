### TODO: Feature Engineering

#### Primary tasks (must-have)
- [ ] Combine training data: `train_real + train_synthetic`
- [ ] Feature engineering → create new features (e.g., ratios, aggregations, time-based features)
- [ ] Encoding categorical variables → numeric (e.g., OneHotEncoder, LabelEncoder)
- [ ] Feature transformation → normalize/skew fixes (e.g., log transform, Box-Cox)
- [ ] Feature scaling → min-max or standardize numeric features
- [ ] Save engineered training features to CSV or joblib for reproducibility

#### Secondary tasks (second essentials)
- [ ] [Supervised ML] Preserve TF-IDF vocabulary/features used by the RandomForest text pipeline so SHAP explains identical dimensions (Tip: dump `tfidf.get_feature_names_out()` with the encoder artifacts)
- [ ] [Unsupervised ML] Assemble the Isolation-Forest feature frame (`Hour`, `Src_IP_LastOctet`, etc.) and record any imputations so runtime anomaly detection feeds match training
- [ ] Optional: Visualize distributions of engineered features (histograms, boxplots) to detect outliers or anomalies

#### Reminder (After splitting)
- [ ] Training Data → Apply full Feature Engineering pipeline → Train Model
- [ ] Validation/Test Data → Apply **same transformations fitted on training data** → Evaluate Model
- [ ] Persist all preprocessing objects (scalers, encoders, transformers) for downstream inference
- [ ] Optionally, after training the final model, train a SHAP explainer on the pipeline using a background sample from training data → Save explainer for interpretability of predictions
